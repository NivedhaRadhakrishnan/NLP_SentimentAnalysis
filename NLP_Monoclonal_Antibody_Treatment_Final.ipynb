{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Monoclonal antibodies (mAbs) are highly effective in treating mild to moderate COVID-19 among nonhospitalized patients. We are asked to perform the following tasks:\n",
    "1. Scrape data from any of the social media platform like facebook, twitter or reddid\n",
    "2. Perform sentiment analysis and label positive and negative comments\n",
    "3. Fetch the key words that are most used across platform\n",
    "4. Fetch the influencers and their geography\n",
    "5. Provide insights obtained using visualization\n",
    "\n",
    "I choose to perform web scraping in twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "\n",
    "1. __<a href='#1' target='_self'>Import Libraries</a>__\n",
    "1. __<a href='#2' target='_self'>Fetch Tweets & Sentiments</a>__\n",
    "    1. __<a href='#2A' target='_self'>Fetch Tweets</a>__\n",
    "    1. __<a href='#2B' target='_self'>Fetch sentiments</a>__\n",
    "1. __<a href='#3' target='_self'>Text Pre-processing</a>__\n",
    "    1. __<a href='#3A' target='_self'>Pre-processing 'Key Words'</a>__\n",
    "        1. <a href='#3Aa' target='_self'>Removing '@names'</a>\n",
    "        1. <a href='#3Ab' target='_self'>Removing links (http | https)</a>\n",
    "        1. <a href='#3Ac' target='_self'>Removing spaces in tweets</a>\n",
    "        1. <a href='#3Ae' target='_self'>Removing Punctuations, Numbers and Special characters</a>\n",
    "        1. <a href='#3Af' target='_self'>Removing Stop words</a>\n",
    "        1. <a href='#3Ag' target='_self'>Tokenizing</a>\n",
    "        1. <a href='#3Ah' target='_self'>Lemmatization </a>\n",
    "        1. <a href='#3i' target='_self'>Joining all tokens into sentences</a>\n",
    "        1. <a href='#3Ad' target='_self'>Dropping redundant rows</a>\n",
    "        1. <a href='#3Ad' target='_self'>Resetting index</a>\n",
    "    1. __<a href='#3B' target='_self'>Pre-processing 'Key Phrases'</a>\n",
    "        1. <a href='#3Ba' target='_self'>Helper class, will help in preprocessing phrase terms</a>\n",
    "        1. <a href='#3Bb' target='_self'>Defining the grammar of the phrases</a>\n",
    "        1. <a href='#3Bc' target='_self'>New feature called 'key_phrases', will contain phrases for corresponding tweet</a>\n",
    "1. __<a href='#4' target='_self'>Story Generation and Visualization</a>__\n",
    "    1. __<a href='#4A' target='_self'>Most common words in positive tweets</a>__\n",
    "    1. __<a href='#4B' target='_self'>Most common words in negative tweets</a>__\n",
    "    1. __<a href='#4C' target='_self'>Most commonly used Hashtags</a>__\n",
    "    1. __<a href='#4D' target='_self'>Most common influencers</a>__\n",
    "1. __<a href='#5' target='_self'>Feature Extraction</a>__\n",
    "    1. __<a href='#5A' target='_self'>Feature Extraction for 'Key Words'</a>__\n",
    "    1. __<a href='#5B' target='_self'>Feature Extraction for 'Key Phrases'</a>__\n",
    "1. __<a href='#6' target='_self'>Model Building: Sentiment Analysis</a>__\n",
    "    1. __<a href='#6A' target='_self'>Predictions on 'key words' based features</a>__\n",
    "        1. <a href='#6Aa' target='_self'> BOW word features</a>\n",
    "        1. <a href='#6Ab' target='_self'>TF-IDF word features</a>\n",
    "    1. __<a href='#6B' target='_self'>Predictions on 'key phrases' based features</a>__\n",
    "        1. <a href='#6Ba' target='_self'>BOW phrase features</a>\n",
    "        1. <a href='#6Bb' target='_self'>TF-IDF phrase features</a>\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1'>1. Import Libraries</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tweepy\n",
    "# pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nivedharakigmail.com/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nivedharakigmail.com/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/nivedharakigmail.com/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nivedharakigmail.com/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/nivedharakigmail.com/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nivedharakigmail.com/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/nivedharakigmail.com/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/nivedharakigmail.com/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Web Scraping\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler \n",
    "\n",
    "# Text mining\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Model Building\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "# Ignoring all the warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Downloading stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('brown')\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# For showing all the plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2'>2. Fetch Tweets & Sentiments</a>\n",
    "### <a id='2A'>A. Fetch Tweets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the Keys and secrets generated from the Twitter Dev platform\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keys and secrets generated from the Twitter Dev platform\n",
    "# consumer_key = input(\"Enter consumer key: \")\n",
    "# consumer_secret = input(\"Enter consumer secret: \")\n",
    "# access_token = input(\"Enter access token: \")\n",
    "# access_token_secret = input(\"Enter access token secret: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a class to fetch twitter API using tweepy library\n",
    "class Fetch_Tweet(object): \n",
    "    #Initialization method \n",
    "    def __init__(self): \n",
    "        try: \n",
    "            # creating OAuth1UserHandler object\n",
    "            auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret,\n",
    "                                            access_token, access_token_secret)\n",
    "            self.api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "            \n",
    "        except tweepy.Unauthorized as e:\n",
    "            print(f\"Error: Authentication Failed - \\n{str(e)}\")\n",
    "\n",
    "    #Defining the function to fetch tweets        \n",
    "    def get_tweets(self, text, maxTweets = 1000): \n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "        sinceId = None\n",
    "        max_id = -1\n",
    "        tweetCnt = 0\n",
    "        tweetsPerTxt = 100\n",
    "\n",
    "        while tweetCnt < maxTweets:\n",
    "            try:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = self.api.search_tweets(q=text, count=tweetsPerTxt)\n",
    "                    else:\n",
    "                        new_tweets = self.api.search_tweets(q=text, count=tweetsPerTxt,\n",
    "                                                since_id=sinceId)\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = self.api.search_tweets(q=text, count=tweetsPerTxt,\n",
    "                                                max_id=str(max_id - 1))\n",
    "                    else:\n",
    "                        new_tweets = self.api.search_tweets(q=text, count=tweetsPerTxt,\n",
    "                                                max_id=str(max_id - 1),\n",
    "                                                since_id=sinceId)\n",
    "                if not new_tweets:\n",
    "                    print(\"End of search\")\n",
    "                    break\n",
    "\n",
    "                for tweet in new_tweets:\n",
    "                    parsed_tweet = {} \n",
    "                    parsed_tweet['tweets'] = tweet.text \n",
    "\n",
    "                    # appending parsed tweet to tweets list \n",
    "                    if tweet.retweet_count > 0: \n",
    "                        # if tweet has retweets, ensure that it is appended only once \n",
    "                        if parsed_tweet not in tweets: \n",
    "                            tweets.append(parsed_tweet) \n",
    "                    else: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                        \n",
    "                tweetCnt += len(new_tweets)\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCnt))\n",
    "                max_id = new_tweets[-1].id\n",
    "\n",
    "            except tweepy.TweepyException as e:\n",
    "                # Terminate program if error occurs\n",
    "                print(\"Tweepy error : \" + str(e))\n",
    "                break\n",
    "        \n",
    "        return pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweepy error : 400 Bad Request\n",
      "215 - Bad Authentication data.\n",
      "tweets_df Shape - (0, 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_client = Fetch_Tweet()\n",
    "\n",
    "# calling function to get tweets\n",
    "tweets_df = twitter_client.get_tweets('monoclonal antibody treatment', maxTweets=9000)\n",
    "print(f'tweets_df Shape - {tweets_df.shape}')\n",
    "tweets_df.to_csv('tweets.csv')\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2B'>B. Fetch sentiments</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching sentiments using Textblob\n",
    "def textblob_fetch_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return 'pos' if analysis.sentiment.polarity >= 0 else 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tweets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2a2f2154d445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtextblob_sentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtextblob_fetch_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextblob_sentiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tweets'"
     ]
    }
   ],
   "source": [
    "textblob_sentiments = tweets_df.tweets.apply(lambda tweet: textblob_fetch_sentiment(tweet))\n",
    "pd.DataFrame(textblob_sentiments.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got 225 positive sentiments and 34 negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['sentiment'] = textblob_sentiments\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3'>3. Text Pre-processing</a> \n",
    "### <a id='3A'>A. Pre-processing Key Words</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pattern(text, pattern_regex):\n",
    "    r = re.findall(pattern_regex, text)\n",
    "    for i in r:\n",
    "        text = i\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(text, pattern_regex):\n",
    "    r = re.findall(pattern_regex, text)\n",
    "    for i in r:\n",
    "        text = re.sub(i, '', text)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column named Influencers with all the names\n",
    "tweets_df['Influencers'] = np.vectorize(add_pattern)(tweets_df['tweets'], \"@[\\w]+\")\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Aa'>a. Removing '@names'</a>\n",
    "Names are not required for analysis hence we remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column named Clean tweets with processed tweets\n",
    "tweets_df['clean_tweets'] = np.vectorize(remove_pattern)(tweets_df['tweets'], \"@[\\w]*: | *RT*\")\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Ab'>b. Removing links from tweets </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "\n",
    "for index, row in tweets_df.iterrows():\n",
    "    words_without_links = [word for word in row.clean_tweets.split() if 'http' or 'https' not in word]\n",
    "    cleaned_tweets.append(' '.join(words_without_links))\n",
    "\n",
    "tweets_df['clean_tweets'] = cleaned_tweets\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Ac'>c. Removing spaces from tweets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[tweets_df['clean_tweets']!='']\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Ad'>d. Removing Punctuations, Numbers and Special characters</a>\n",
    "Since the semantics are required during the sentiment analysis of Key phrases we create a new column called 'clean_tweets_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['clean_tweets_final'] = tweets_df['clean_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Ae'>e. Removing Stop words</a>\n",
    "Stop words don't contribute to the analysis and hence are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords)\n",
    "cleaned_tweets = []\n",
    "\n",
    "for index, row in tweets_df.iterrows():\n",
    "    \n",
    "    # filerting out all the stopwords \n",
    "    words_without_stopwords = [word for word in row.clean_tweets_final.split() if not word in stopwords_set and '#' not in word.lower()]\n",
    "    \n",
    "    # finally creating tweets list of tuples containing stopwords(list) and sentimentType \n",
    "    cleaned_tweets.append(' '.join(words_without_stopwords))\n",
    "    \n",
    "tweets_df['clean_tweets_final'] = cleaned_tweets\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Af'>f. Tokenize *'clean_tweets_final'*</a> \n",
    "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet = tweets_df['clean_tweets_final'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Ag'>g. Lemmatization </a>\n",
    "Lemmatization is a common normalization technique in text pre-processing. In lemmatization, words are replaced by their root form or words with similar context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Ah'>h. Joining all tokens into sentences</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tokens in enumerate(tokenized_tweet):\n",
    "    tokenized_tweet[i] = ' '.join(tokens)\n",
    "\n",
    "tweets_df['clean_tweets_final'] = tokenized_tweet\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Ai'>i. Dropping redundant rows</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop_duplicates(subset=['clean_tweets'], keep=False)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Aj'>j. Resetting index</a>\n",
    "It seems that our index needs to be reset, since after removal of some rows, some index values are missing, which may cause problem in future operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.reset_index(drop=True)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3B'>B. Pre-processing 'Key Phrases'</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Ba'>a. Helper class, will help in preprocessing phrase terms</a>\n",
    "The pre-processing techniques used are :\n",
    "1. Lemmatization\n",
    "2. Stemming. Stemming is a natural language processing technique that lowers inflection in words to their root forms\n",
    "3. Case Normalization\n",
    "4. Optimize length of phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseExtractHelper(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.WordNetLemmatizer()\n",
    "        self.stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    # Finds NP (nounphrase) leaf nodes of a chunk tree\n",
    "    def leaves(self, tree):\n",
    "        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "            yield subtree.leaves()\n",
    "            \n",
    "    #Normalises words to lowercase and stems and lemmatizes it\n",
    "    def normalise(self, word): \n",
    "        word = word.lower()\n",
    "        word = self.lemmatizer.lemmatize(word)\n",
    "        return word\n",
    "    #Checks conditions for acceptable word: length, stopword. We can increase the length if we want to consider large phrase\n",
    "    def acceptable_word(self, word):\n",
    "        accepted = bool(3 <= len(word) <= 40\n",
    "            and word.lower() not in stopwords\n",
    "            and 'https' not in word.lower()\n",
    "            and 'http' not in word.lower()\n",
    "            and '#' not in word.lower()\n",
    "            )\n",
    "        return accepted\n",
    "\n",
    "    def get_terms(self, tree):\n",
    "        for leaf in self.leaves(tree):\n",
    "            term = [ self.normalise(w) for w,t in leaf if self.acceptable_word(w) ]\n",
    "            yield term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Bb'>b. Defining the grammar of the phrases</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "\"\"\"\n",
    "chunker = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='3Bc'>c. New feature called 'key_phrases', will contain phrases for corresponding tweet</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrases = []\n",
    "phrase_extract_helper = PhraseExtractHelper()\n",
    "\n",
    "for index, row in tweets_df.iterrows(): \n",
    "    toks = nltk.regexp_tokenize(row.clean_tweets, sentence_re)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "    tree = chunker.parse(postoks)\n",
    "\n",
    "    terms = phrase_extract_helper.get_terms(tree)\n",
    "    tweet_phrases = []\n",
    "\n",
    "    for term in terms:\n",
    "        if len(term):\n",
    "            tweet_phrases.append(' '.join(term))\n",
    "    \n",
    "    key_phrases.append(tweet_phrases)\n",
    "    \n",
    "key_phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_key_phrases = []\n",
    "extractor = ConllExtractor()\n",
    "\n",
    "for index, row in tweets_df.iterrows():\n",
    "    # filerting out all the hashtags\n",
    "    words_without_hash = [word for word in row.clean_tweets.split() if '#' not in word.lower()]\n",
    "    \n",
    "    hash_removed_sentence = ' '.join(words_without_hash)\n",
    "    \n",
    "    blob = TextBlob(hash_removed_sentence, np_extractor=extractor)\n",
    "    textblob_key_phrases.append(list(blob.noun_phrases))\n",
    "\n",
    "textblob_key_phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['key_phrases'] = textblob_key_phrases\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4'>4. Story Generation and Visualization</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate wordcloud\n",
    "def generate_wordcloud(all_words):\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect hashtags\n",
    "def hashtag_extract(text_list):\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for text in text_list:\n",
    "        ht = re.findall(r\"#(\\w+)\", text)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    return hashtags\n",
    "\n",
    "def generate_hashtag_freqdist(hashtags):\n",
    "    a = nltk.FreqDist(hashtags)\n",
    "    d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                      'Count': list(a.values())})\n",
    "    # selecting top 15 most frequent hashtags     \n",
    "    d = d.nlargest(columns=\"Count\", n = 20)\n",
    "    plt.figure(figsize=(16,7))\n",
    "    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "    plt.xticks(rotation=80)\n",
    "    ax.set(ylabel = 'Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='4A'>A. Most common words in positive tweets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in tweets_df['clean_tweets_final'][tweets_df.sentiment == 'pos']])\n",
    "generate_wordcloud(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wordcloud shows that the positive key words are monoclonal antibody,treatment,Prion protein,Covid,drug,antiviral theraphy etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='4B'>B. Most common words in negative tweets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in tweets_df['clean_tweets_final'][tweets_df.sentiment == 'neg']])\n",
    "generate_wordcloud(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wordcloud shows that the negative key words are monoclonal antibody,covid,vacine hesitant,skeptical,prevent,immuno compromised,letters etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='4C'>C. Most commonly used Hashtags</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = hashtag_extract(tweets_df['clean_tweets'])\n",
    "hashtags = sum(hashtags, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_hashtag_freqdist(hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the histogram of the hastags that are commonly used. We can see the presence of drugs like Evusheld, Paxlovid, Medvix. We can also see the variants of Covid19 like Omicron. The mention of passive genocide reflects the disappointment of the third world contries. Florida comes under the geographical factor in the key topic of discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='4D'>D. Most common influencers </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_hashtag_freqdist(tweets_df['Influencers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top influencers are tagged by the name B52malmet, Sophos_Veritate, theLancetNeuro, CDRMaguire and so on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sake of consistency, we are going to discard the records which contains no phrases i.e where tweets_df['key_phrases'] contains []\n",
    "tweets_df2 = tweets_df[tweets_df['key_phrases'].str.len()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='5'>5. Feature Extraction</a>\n",
    "\n",
    "Feature extraction can be done by two methods. They are as follows:\n",
    "\n",
    "1. __Bag of words (Simple vectorization)__\n",
    "2. __TF-IDF (Term Frequency - Inverse Document Frequency)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='5A'>A. Feature Extraction for 'Key Words'</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW features\n",
    "bow_word_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow_word_feature = bow_word_vectorizer.fit_transform(tweets_df2['clean_tweets_final'])\n",
    "\n",
    "# TF-IDF features\n",
    "tfidf_word_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf_word_feature = tfidf_word_vectorizer.fit_transform(tweets_df2['clean_tweets_final'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='5B'>B. Feature Extraction for 'Key Phrases'</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_sents = tweets_df2['key_phrases'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# BOW phrase features\n",
    "bow_phrase_vectorizer = CountVectorizer(max_df=0.90, min_df=2)\n",
    "bow_phrase_feature = bow_phrase_vectorizer.fit_transform(phrase_sents)\n",
    "\n",
    "# TF-IDF phrase feature\n",
    "tfidf_phrase_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2)\n",
    "tfidf_phrase_feature = tfidf_phrase_vectorizer.fit_transform(phrase_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='6'>6. Model Building: Sentiment Analysis</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping target variables to  {0, 1}\n",
    "target_variable = tweets_df2['sentiment'].apply(lambda x: 0 if x=='neg' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix):\n",
    "    plt.clf()\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Set2_r)\n",
    "    classNames = ['Positive', 'Negative']\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Actual')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames)\n",
    "    plt.yticks(tick_marks, classNames)\n",
    "    s = [['TP','FP'], ['FN', 'TN']]\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(matrix[i][j]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model(X_train, X_test, y_train, y_test):\n",
    "    naive_classifier = GaussianNB()\n",
    "    naive_classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "    # predictions over test set\n",
    "    predictions = naive_classifier.predict(X_test.toarray())\n",
    "\n",
    "    # calculating Accuracy Score\n",
    "    print(f'Accuracy Score - {accuracy_score(y_test, predictions)}')\n",
    "    conf_matrix = confusion_matrix(y_test, predictions, labels=[True, False])\n",
    "    plot_confusion_matrix(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='6A'>A. Predictions on 'key words' based features</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='6Aa'>a. BOW word features</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_word_feature, target_variable, test_size=0.3, random_state=272)\n",
    "naive_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='6Ab'>b. TF-IDF word features</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_word_feature, target_variable, test_size=0.3, random_state=272)\n",
    "naive_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='6B'>B. Predictions on 'key phrases' based features</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='6Ba'>a. BOW Phrase features</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_phrase_feature, target_variable, test_size=0.3, random_state=272)\n",
    "naive_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='6Bb'>b. TF-IDF Phrase features</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_phrase_feature, target_variable, test_size=0.3, random_state=272)\n",
    "naive_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the accuracy score features extracted from 'key words' helps model perform better. They give better positive predictions than the features extracted from 'key phrases'. Between BOW and TF-IDF, BOW gives better result. So we choose BOW and key words based model using NaiveBayes classifier.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
